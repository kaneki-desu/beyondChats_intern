{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eac74404",
   "metadata": {},
   "outputs": [],
   "source": [
    "username=\"kojied\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b78f1cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_fetcher import fetch_user_posts, fetch_user_comments\n",
    "from reddit_client import reddit\n",
    "import pandas as pd\n",
    "user = reddit.redditor(username)\n",
    "posts=fetch_user_posts(user, limit=None)\n",
    "comments=fetch_user_comments(user, limit=None)\n",
    "activity_df = pd.DataFrame(posts + comments)\n",
    "activity_df.to_csv(f\"Data/{username}_activity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eca3698a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sibaj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sibaj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\sibaj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running topic modeling...\n",
      "‚úÖ Qualitative persona saved to qualitative_persona.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from datetime import datetime\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nrclex import NRCLex\n",
    "from detoxify import Detoxify\n",
    "from bertopic import BERTopic\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# === NLTK Resources ===\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "# === Load & Clean ===\n",
    "df = pd.read_csv(\"Data/kojied_activity.csv\")\n",
    "df[\"text\"] = df[\"text\"].fillna(\"\")\n",
    "df[\"title\"] = df[\"title\"].fillna(\"\")\n",
    "df[\"content\"] = df[\"title\"] + \". \" + df[\"text\"]\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# === Sentiment & Emotion ===\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "df[\"sentiment_score\"] = df[\"content\"].apply(lambda x: vader.polarity_scores(x)[\"compound\"])\n",
    "df[\"toxicity\"] = df[\"content\"].apply(lambda x: Detoxify(\"original\").predict(x)[\"toxicity\"])\n",
    "\n",
    "def extract_emotion(text):\n",
    "    emotions = NRCLex(text).top_emotions\n",
    "    return emotions[0] if emotions else (\"neutral\", 0)\n",
    "\n",
    "df[[\"top_emotion\", \"emotion_score\"]] = df[\"content\"].apply(lambda x: pd.Series(extract_emotion(x)))\n",
    "\n",
    "# === Topic Modeling ===\n",
    "print(\"Running topic modeling...\")\n",
    "topic_model = BERTopic(verbose=False)\n",
    "topics, _ = topic_model.fit_transform(df[\"content\"].tolist())\n",
    "df[\"topic\"] = topics\n",
    "\n",
    "# === Tokenize for Trait Analysis ===\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    return [w for w in tokens if w.isalpha() and w not in stop_words]\n",
    "\n",
    "df[\"tokens\"] = df[\"content\"].apply(preprocess)\n",
    "tokens = [word for row in df[\"tokens\"] for word in row]\n",
    "word_counts = Counter(tokens)\n",
    "\n",
    "# === Big Five Traits (OCEAN) ===\n",
    "OCEAN = {\n",
    "    \"openness\": {\"imagine\", \"explore\", \"curious\", \"creative\", \"adventure\", \"philosophy\", \"novel\"},\n",
    "    \"conscientiousness\": {\"plan\", \"organize\", \"routine\", \"discipline\", \"goal\", \"work\", \"task\"},\n",
    "    \"extraversion\": {\"party\", \"fun\", \"people\", \"chat\", \"friends\", \"energy\", \"meet\"},\n",
    "    \"agreeableness\": {\"kind\", \"thank\", \"help\", \"please\", \"cooperate\", \"gentle\", \"appreciate\"},\n",
    "    \"neuroticism\": {\"worry\", \"anxious\", \"sad\", \"angry\", \"regret\", \"fear\", \"nervous\"},\n",
    "}\n",
    "trait_scores = {trait: sum(word_counts.get(w, 0) for w in words) for trait, words in OCEAN.items()}\n",
    "total_words = len(tokens)\n",
    "trait_normalized = {k: round(v / total_words, 4) for k, v in trait_scores.items()}\n",
    "\n",
    "# === MBTI ===\n",
    "MBTI = {\"I\": 0, \"E\": 0, \"S\": 0, \"N\": 0, \"T\": 0, \"F\": 0, \"J\": 0, \"P\": 0}\n",
    "patterns = {\n",
    "    \"I\": {\"alone\", \"quiet\", \"reflect\"},\n",
    "    \"E\": {\"chat\", \"talk\", \"meet\", \"group\"},\n",
    "    \"S\": {\"facts\", \"detail\", \"experience\"},\n",
    "    \"N\": {\"imagine\", \"abstract\", \"dream\", \"future\"},\n",
    "    \"T\": {\"think\", \"logic\", \"reason\"},\n",
    "    \"F\": {\"feel\", \"care\", \"emotion\"},\n",
    "    \"J\": {\"plan\", \"organized\", \"decide\"},\n",
    "    \"P\": {\"flexible\", \"explore\", \"spontaneous\"},\n",
    "}\n",
    "for key, words in patterns.items():\n",
    "    MBTI[key] = sum(word_counts.get(w, 0) for w in words)\n",
    "mbti_final = (\n",
    "    \"I\" if MBTI[\"I\"] >= MBTI[\"E\"] else \"E\"\n",
    ") + (\n",
    "    \"S\" if MBTI[\"S\"] >= MBTI[\"N\"] else \"N\"\n",
    ") + (\n",
    "    \"T\" if MBTI[\"T\"] >= MBTI[\"F\"] else \"F\"\n",
    ") + (\n",
    "    \"J\" if MBTI[\"J\"] >= MBTI[\"P\"] else \"P\"\n",
    ")\n",
    "\n",
    "# === Find Extremes ===\n",
    "most_pos = df.loc[df[\"sentiment_score\"].idxmax()]\n",
    "most_neg = df.loc[df[\"sentiment_score\"].idxmin()]\n",
    "most_toxic = df.loc[df[\"toxicity\"].idxmax()]\n",
    "top_emotion = df.loc[df[\"emotion_score\"].idxmax()]\n",
    "\n",
    "# === Extract Age, Occupation from text if possible ===\n",
    "def extract_age(text):\n",
    "    match = re.search(r\"\\b(\\d{1,2})\\s*(years old|yo)\\b\", text.lower())\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def extract_occupation(text):\n",
    "    jobs = [\"student\", \"developer\", \"designer\", \"engineer\", \"teacher\", \"artist\", \"manager\"]\n",
    "    for word in text.lower().split():\n",
    "        if word in jobs:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "age = next((extract_age(c) for c in df[\"content\"] if extract_age(c)), \"Unknown\")\n",
    "occupation = next((extract_occupation(c) for c in df[\"content\"] if extract_occupation(c)), \"Unknown\")\n",
    "\n",
    "# === Save Qualitative Persona ===\n",
    "with open(\"UserProfile/qualitative_persona.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== Qualitative Persona ===\\n\")\n",
    "    f.write(f\"Generated: {datetime.utcnow().isoformat()}\\n\\n\")\n",
    "    f.write(f\"üßë Name: {df.get('author', pd.Series(['Unknown'])).iloc[0]}\\n\")\n",
    "    f.write(f\"üìç Age: {age or 'Unknown'}\\n\")\n",
    "    f.write(f\"üíº Occupation: {occupation or 'Unknown'}\\n\")\n",
    "    f.write(\"üåç Location: Unknown\\n\\n\")\n",
    "\n",
    "    f.write(\"üìù Background:\\n\")\n",
    "    f.write(\"User appears active on Reddit, posting about diverse themes like \")\n",
    "    top_words = [word for word, _ in word_counts.most_common(5)]\n",
    "    f.write(\", \".join(top_words) + \".\\n\\n\")\n",
    "\n",
    "    f.write(\"üéØ Goals & Needs:\\n\")\n",
    "    f.write(\"- Personal growth and self-improvement.\\n\" if \"improve\" in tokens else \"- Unknown\\n\")\n",
    "    f.write(\"- Possibly career-focused.\\n\" if \"work\" in tokens or \"study\" in tokens else \"\")\n",
    "    f.write(\"\\n\\n\")\n",
    "\n",
    "    f.write(\"üò£ Pain Points:\\n\")\n",
    "    f.write(f\"- \\\"{most_neg['content'][:100]}...\\\"\\n\" if most_neg[\"sentiment_score\"] < -0.4 else \"- Not strongly negative.\\n\")\n",
    "    f.write(f\"- \\\"{most_toxic['content'][:100]}...\\\"\\n\" if most_toxic[\"toxicity\"] > 0.5 else \"\")\n",
    "    f.write(\"\\n\\n\")\n",
    "\n",
    "    f.write(\"üß† Personality Traits (OCEAN):\\n\")\n",
    "    for trait, score in trait_normalized.items():\n",
    "        f.write(f\"- {trait.title()}: {score}\\n\")\n",
    "    f.write(f\"- MBTI Estimate: {mbti_final}\\n\\n\")\n",
    "\n",
    "    f.write(\"üí° Motivations:\\n\")\n",
    "    if top_emotion[\"emotion_score\"] > 0.3:\n",
    "        f.write(f\"- Dominant Emotion: {top_emotion['top_emotion']} ‚Äì \\\"{top_emotion['content'][:80]}...\\\"\\n\")\n",
    "    else:\n",
    "        f.write(\"- No strong emotion found.\\n\")\n",
    "\n",
    "    f.write(\"\\nüì± Tech Behavior:\\n\")\n",
    "    f.write(\"- Uses Reddit actively. Mentions YouTube, Instagram if found.\\n\")\n",
    "    if \"reddit\" in tokens:\n",
    "        f.write(\"- Engages in online discussions.\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"üó£Ô∏è Representative Quote:\\n\")\n",
    "    f.write(f\"\\\"{most_pos['content'][:180]}...\\\"\\n\")\n",
    "\n",
    "print(\"‚úÖ Qualitative persona saved to qualitative_persona.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a74f741b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\sibaj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sibaj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2025-07-15 21:51:38,368 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic topic modeling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:02<00:00,  4.41it/s]\n",
      "2025-07-15 21:51:44,916 - BERTopic - Embedding - Completed ‚úì\n",
      "2025-07-15 21:51:44,916 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-07-15 21:51:45,145 - BERTopic - Dimensionality - Completed ‚úì\n",
      "2025-07-15 21:51:45,147 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-07-15 21:51:45,166 - BERTopic - Cluster - Completed ‚úì\n",
      "2025-07-15 21:51:45,169 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-07-15 21:51:45,219 - BERTopic - Representation - Completed ‚úì\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Profile saved to persona_profile.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from detoxify import Detoxify\n",
    "from nrclex import NRCLex\n",
    "from bertopic import BERTopic\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# === Load CSV ===\n",
    "df = pd.read_csv(\"Data/kojied_activity.csv\")  # Replace with your filename\n",
    "df[\"text\"] = df[\"text\"].fillna(\"\")  # Ensure no NaN\n",
    "\n",
    "# === Combine title and text for better topic modeling ===\n",
    "df[\"content\"] = df[\"title\"].fillna(\"\") + \". \" + df[\"text\"].fillna(\"\")\n",
    "\n",
    "# === Prepare a helper to fetch ID ===\n",
    "def get_id(row):\n",
    "    return row[\"id\"]\n",
    "\n",
    "# === Sentiment Analysis ===\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "df[\"sentiment_score\"] = df[\"content\"].apply(lambda x: vader.polarity_scores(str(x))[\"compound\"])\n",
    "\n",
    "# === Toxicity Detection ===\n",
    "df[\"toxicity\"] = df[\"content\"].apply(lambda x: Detoxify(\"original\").predict(str(x))[\"toxicity\"])\n",
    "\n",
    "# === Emotion Detection ===\n",
    "def top_emotion(text):\n",
    "    emotions = NRCLex(text).top_emotions\n",
    "    return emotions[0] if emotions else (\"neutral\", 0)\n",
    "\n",
    "df[[\"top_emotion\", \"emotion_score\"]] = df[\"content\"].apply(lambda x: pd.Series(top_emotion(x)))\n",
    "\n",
    "# === Topic Modeling ===\n",
    "print(\"Running BERTopic topic modeling...\")\n",
    "topic_model = BERTopic(verbose=True)\n",
    "topics, _ = topic_model.fit_transform(df[\"content\"].tolist())\n",
    "df[\"topic\"] = topics\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "# === Extract Most Relevant Posts/Comments Per Trait ===\n",
    "most_pos = df.loc[df[\"sentiment_score\"].idxmax()]\n",
    "most_neg = df.loc[df[\"sentiment_score\"].idxmin()]\n",
    "most_toxic = df.loc[df[\"toxicity\"].idxmax()]\n",
    "\n",
    "emotions = {}\n",
    "for emotion in df[\"top_emotion\"].unique():\n",
    "    top_row = df[df[\"top_emotion\"] == emotion].sort_values(\"emotion_score\", ascending=False).head(1)\n",
    "    if not top_row.empty:\n",
    "        top = top_row.iloc[0]\n",
    "        emotions[emotion] = {\n",
    "            \"text\": top[\"content\"],\n",
    "            \"id\": get_id(top),\n",
    "            \"score\": round(top[\"emotion_score\"], 3)\n",
    "        }\n",
    "\n",
    "topic_examples = {}\n",
    "for topic_id in df[\"topic\"].unique():\n",
    "    if topic_id == -1:\n",
    "        continue\n",
    "    top_topic = df[df[\"topic\"] == topic_id].sort_values(\"score\", ascending=False).head(1)\n",
    "    if not top_topic.empty:\n",
    "        top = top_topic.iloc[0]\n",
    "        topic_examples[str(topic_id)] = {\n",
    "            \"topic_words\": topic_model.get_topic(topic_id),\n",
    "            \"example_text\": top[\"content\"],\n",
    "            \"id\": get_id(top),\n",
    "            \"score\": top[\"score\"]\n",
    "        }\n",
    "\n",
    "# === Save to TXT instead of JSON ===\n",
    "with open(\"UserProfile/persona_profile.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== Persona Profile Report ===\\n\")\n",
    "    f.write(f\"Generated on: {datetime.utcnow().isoformat()}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Posts Analyzed: {len(df)}\\n\\n\")\n",
    "\n",
    "    # Sentiment\n",
    "    f.write(\">> Sentiment Analysis:\\n\")\n",
    "    f.write(f\"Average Sentiment Score: {round(df['sentiment_score'].mean(), 3)}\\n\")\n",
    "    f.write(f\"Most Positive (Score {most_pos['sentiment_score']}): [{get_id(most_pos)}]\\n{most_pos['content']}\\n\\n\")\n",
    "    f.write(f\"Most Negative (Score {most_neg['sentiment_score']}): [{get_id(most_neg)}]\\n{most_neg['content']}\\n\\n\")\n",
    "\n",
    "    # Toxicity\n",
    "    f.write(\">> Toxicity:\\n\")\n",
    "    f.write(f\"Average Toxicity Score: {round(df['toxicity'].mean(), 3)}\\n\")\n",
    "    f.write(f\"Most Toxic (Score {most_toxic['toxicity']}): [{get_id(most_toxic)}]\\n{most_toxic['content']}\\n\\n\")\n",
    "\n",
    "    # Emotions\n",
    "    f.write(\">> Emotion Highlights:\\n\")\n",
    "    for emotion, details in emotions.items():\n",
    "        f.write(f\"{emotion.title()} (Score {details['score']}): [{details['id']}]\\n{details['text']}\\n\\n\")\n",
    "\n",
    "    # Topics\n",
    "    f.write(\">> Topic Insights:\\n\")\n",
    "    for topic_id, details in topic_examples.items():\n",
    "        f.write(f\"Topic {topic_id} Keywords: {', '.join([word for word, _ in details['topic_words']])}\\n\")\n",
    "        f.write(f\"Top Example (Score {details['score']}): [{details['id']}]\\n{details['example_text']}\\n\\n\")\n",
    "\n",
    "print(\"‚úÖ Profile saved to persona_profile.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f7f3f551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sibaj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sibaj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extended behavior report saved to behavior_profile.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# === Load CSV ===\n",
    "df = pd.read_csv(\"Data/kojied_activity.csv\")\n",
    "df[\"text\"] = df[\"text\"].fillna(\"\")\n",
    "df[\"content\"] = df[\"title\"].fillna(\"\") + \". \" + df[\"text\"].fillna(\"\")\n",
    "\n",
    "# === Preprocessing ===\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(str(text).lower())\n",
    "    return [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "\n",
    "df[\"tokens\"] = df[\"content\"].apply(preprocess)\n",
    "\n",
    "# === Define Behavior Categories ===\n",
    "behaviors = {\n",
    "    \"work_study\": {\"study\", \"exam\", \"task\", \"goal\", \"work\", \"deadline\", \"project\", \"submit\", \"assignment\", \"focus\"},\n",
    "    \"home_life\": {\"home\", \"family\", \"clean\", \"cook\", \"chores\", \"house\", \"room\", \"kitchen\", \"laundry\"},\n",
    "    \"entertainment\": {\"movie\", \"anime\", \"game\", \"binge\", \"netflix\", \"series\", \"watch\", \"film\", \"tv\", \"play\", \"meme\"},\n",
    "    \"social_life\": {\"party\", \"hangout\", \"friend\", \"group\", \"talk\", \"meet\", \"invite\", \"event\"},\n",
    "    \"values_beliefs\": {\"god\", \"faith\", \"religion\", \"ethics\", \"moral\", \"belief\", \"spiritual\"},\n",
    "    \"health_fitness\": {\"gym\", \"workout\", \"exercise\", \"diet\", \"run\", \"protein\", \"calorie\", \"fitness\", \"training\"},\n",
    "    \"brand_behavior\": {\"nike\", \"apple\", \"amazon\", \"brand\", \"shop\", \"purchase\", \"wear\", \"logo\", \"store\"},\n",
    "    \"online_activity\": {\"reddit\", \"youtube\", \"scroll\", \"comment\", \"post\", \"stream\", \"instagram\", \"social\", \"media\"},\n",
    "    \"travel\": {\"trip\", \"travel\", \"flight\", \"train\", \"road\", \"vacation\", \"passport\", \"journey\", \"airport\", \"adventure\"},\n",
    "    \"self_improvement\": {\"motivation\", \"growth\", \"improve\", \"discipline\", \"change\", \"habit\", \"learn\"},\n",
    "    \"emotions\": {\"happy\", \"sad\", \"angry\", \"regret\", \"anxious\", \"depressed\", \"excited\", \"lonely\"},\n",
    "    \"finance\": {\"money\", \"buy\", \"spend\", \"save\", \"budget\", \"stock\", \"invest\", \"purchase\", \"salary\", \"debt\", \"bank\"},\n",
    "    \"relationships\": {\"love\", \"relationship\", \"girlfriend\", \"boyfriend\", \"dating\", \"crush\", \"romantic\", \"partner\", \"ex\"},\n",
    "    \"daily_routine\": {\"breakfast\", \"coffee\", \"sleep\", \"nap\", \"wake\", \"bed\", \"dream\", \"morning\", \"night\", \"alarm\"},\n",
    "    \"identity_expression\": {\"gender\", \"race\", \"lgbtq\", \"identity\", \"culture\", \"indian\", \"asian\", \"american\"},\n",
    "}\n",
    "\n",
    "# === Analyze Behavior ===\n",
    "behavior_matches = defaultdict(list)\n",
    "behavior_scores = defaultdict(int)\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    tokens = set(row[\"tokens\"])\n",
    "    for category, keywords in behaviors.items():\n",
    "        match_count = len(tokens & keywords)\n",
    "        if match_count > 0:\n",
    "            behavior_scores[category] += match_count\n",
    "            behavior_matches[category].append((row[\"id\"], row[\"content\"], match_count))\n",
    "\n",
    "# === Normalize Frequencies ===\n",
    "total_hits = sum(behavior_scores.values())\n",
    "behavior_normalized = {\n",
    "    k: round(v / total_hits, 3) if total_hits > 0 else 0\n",
    "    for k, v in behavior_scores.items()\n",
    "}\n",
    "\n",
    "# === Sort & select top 3 examples per category ===\n",
    "top_examples = {\n",
    "    category: sorted(posts, key=lambda x: -x[2])[:3]\n",
    "    for category, posts in behavior_matches.items()\n",
    "}\n",
    "\n",
    "# === Save to TXT Report ===\n",
    "with open(\"UserProfile/behavior_profile.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== Behavior & Habit Profile ===\\n\")\n",
    "    f.write(f\"Generated: {datetime.utcnow().isoformat()}\\n\")\n",
    "    f.write(f\"Posts Analyzed: {len(df)}\\n\\n\")\n",
    "\n",
    "    f.write(\">> Behavioral Category Frequency (normalized):\\n\")\n",
    "    for category in sorted(behavior_normalized, key=behavior_normalized.get, reverse=True):\n",
    "        f.write(f\"{category.replace('_', ' ').title():<22}: {behavior_normalized[category]}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    for category in sorted(behaviors.keys()):\n",
    "        f.write(f\">> {category.upper()} ({behavior_scores[category]} matches)\\n\")\n",
    "        examples = top_examples.get(category, [])\n",
    "        if examples:\n",
    "            for i, (id_val, content, score) in enumerate(examples, 1):\n",
    "                f.write(f\"  [{i}] ID: {id_val} | Score: {score}\\n\")\n",
    "                f.write(f\"      {content[:300].strip()}...\\n\")\n",
    "        else:\n",
    "            f.write(\"  No strong matches found.\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"‚úÖ Extended behavior report saved to behavior_profile.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20e8f569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Combined profile saved to: UserProfile\\combined_user_profile.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Folder path\n",
    "folder = Path(\"UserProfile\")\n",
    "\n",
    "# Files to combine\n",
    "files = [\"qualitative_persona.txt\", \"persona_profile.txt\", \"behavior_profile.txt\"]\n",
    "output_file = folder / \"combined_user_profile.txt\"\n",
    "\n",
    "# Combine contents\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as out:\n",
    "    out.write(\"=== COMBINED USER PROFILE ===\\n\")\n",
    "    out.write(f\"Generated: {datetime.utcnow().isoformat()}\\n\\n\")\n",
    "    \n",
    "    for filename in files:\n",
    "        file_path = folder / filename\n",
    "        if file_path.exists():\n",
    "            out.write(f\"\\n\\n--- {filename.upper()} ---\\n\\n\")\n",
    "            content = file_path.read_text(encoding=\"utf-8\")\n",
    "            out.write(content.strip())\n",
    "        else:\n",
    "            out.write(f\"\\n\\n--- {filename.upper()} NOT FOUND ---\\n\")\n",
    "\n",
    "print(f\"‚úÖ Combined profile saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "869c1a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Reddit_JSON_Formatter_Agent, Reddit_Personality_agent\n",
    "import textwrap\n",
    "\n",
    "CHUNK_SIZE = 5500  # Adjust as per agent's token limits and safety margin\n",
    "\n",
    "async def convert_persona_to_json(markdown_persona: str):\n",
    "    print(\"[+] Splitting persona into manageable chunks...\")\n",
    "\n",
    "    # Split the input into chunks\n",
    "    chunks = textwrap.wrap(markdown_persona, CHUNK_SIZE, break_long_words=False, break_on_hyphens=False)\n",
    "\n",
    "    json_prompt_base = \"\"\"\n",
    "You will be given a user persona written in markdown with section headings such as Biodata, Motivations, Personality, etc.\n",
    "In \"AI_insigts\", you explain how you got the corresponding attributes details and they must contain ids of those posts or comments.\n",
    "Each AI_insights must be back by the post id's . If you are guessing some attributes , write how you guessed it in AI_insights if possible with ids.\n",
    "You can add new attributes in personality , motivation.\n",
    "Please double-check each word , each commas, each bracket ,each literals and characters before giving the final output as it gets difficult to debug in frontend\n",
    "\"\"\"\n",
    "\n",
    "    json_schema_header = \"\"\"\n",
    "#json\n",
    "{\n",
    "  \"persona\": {\n",
    "    \"reddit_username\": \"{username}\",\n",
    "    \"name\": null,\n",
    "    \"photo_url\": \"\",\n",
    "    \"demographics\": {\n",
    "      \"age\": null,\n",
    "      \"occupation\": null,\n",
    "      \"location\": null,\n",
    "      \"marital_status\": null,\n",
    "      \"tier\": null,\n",
    "      \"archetype\": null,\n",
    "      \"AI_insights\": \"\"\n",
    "    },\n",
    "    \"quote\": \"\",\n",
    "    \"traits\": {\n",
    "      \"content\": [],\n",
    "      \"AI_insights\": \"\"\n",
    "    },\n",
    "    \"motivations\": {\n",
    "      \"content\": [],\n",
    "      \"AI_insights\": \"\"\n",
    "    },\n",
    "    \"personality\": {\n",
    "      \"content\": [],\n",
    "      \"AI_insights\": \"\"\n",
    "    },\n",
    "    \"behaviors_habits\": {\n",
    "      \"content\": [],\n",
    "      \"AI_insights\": \"\"\n",
    "    },\n",
    "    \"goals_needs\": {\n",
    "      \"content\": [],\n",
    "      \"AI_insights\": \"\"\n",
    "    },\n",
    "    \"pain_points\": {\n",
    "      \"content\": [],\n",
    "      \"AI_insights\": \"\"\n",
    "    },\n",
    "    \"tools_technology\": {\n",
    "      \"content\": [],\n",
    "      \"AI_insights\": \"\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "    final_output = \"\"\n",
    "\n",
    "    # Process each chunk\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        print(f\"[+] Processing chunk {idx + 1}/{len(chunks)}...\")\n",
    "        full_prompt = f\"{json_prompt_base}\\n\\nSchema:\\n{json_schema_header}\\n\\nChunk {idx + 1}:\\n{chunk}\\n\\nReturn only JSON.\"\n",
    "        response = await Reddit_Personality_agent.arun(full_prompt)\n",
    "        final_output += response.content.strip() + \"\\n\"\n",
    "    json_prompt_base=\"\"\"\n",
    "Combine all these into a single json like structured below\n",
    "In \"AI_insigts\", you explain how you got the corresponding attributes details and they must contain ids of those posts or comments.\n",
    "Each AI_insights must be back by the post id's . If you are guessing some attributes , write how you guessed it in AI_insights if possible with ids.\n",
    "You can add new attributes in personality , motivation.\n",
    "Please double-check each word , each commas, each bracket ,each literals and characters before giving the final output as it gets difficult to debug in frontend\"\"\"\n",
    "    json_schema_header = \"\"\"\n",
    "#json\n",
    "{\n",
    "  \"persona\": {\n",
    "    \"reddit_username\": \"{username}\",\n",
    "    \"name\": null,\n",
    "    \"photo_url\": \"\",\n",
    "    \"demographics\": {\n",
    "      \"age\": null,\n",
    "      \"occupation\": null,\n",
    "      \"location\": null,\n",
    "      \"marital_status\": null,\n",
    "      \"tier\": null,\n",
    "      \"archetype\": null,\n",
    "      \"AI_insights\": \"\"\n",
    "    },\n",
    "    \"quote\": \"\",\n",
    "    \"traits\": {\n",
    "      \"content\": [],\n",
    "      \"AI_insights\": \"\"\n",
    "    },\n",
    "    \"motivations\": {\n",
    "      \"content\": [],\n",
    "      \"AI_insights\": \"\"\n",
    "    },\n",
    "    \"personality\": {\n",
    "      \"content\": [],\n",
    "      \"AI_insights\": \"\"\n",
    "    },\n",
    "    \"behaviors_habits\": {\n",
    "      \"content\": [],\n",
    "      \"AI_insights\": \"\"\n",
    "    },\n",
    "    \"goals_needs\": {\n",
    "      \"content\": [],\n",
    "      \"AI_insights\": \"\"\n",
    "    },\n",
    "    \"pain_points\": {\n",
    "      \"content\": [],\n",
    "      \"AI_insights\": \"\"\n",
    "    },\n",
    "    \"tools_technology\": {\n",
    "      \"content\": [],\n",
    "      \"AI_insights\": \"\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "    final_prompt= f\"{json_prompt_base}\\n\\nSchema:\\n{json_schema_header}\\n\\nReturn only a single JSON.\"\n",
    "    final_persona=await Reddit_JSON_Formatter_Agent.arun(final_prompt)\n",
    "\n",
    "    return final_persona\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8657a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Splitting persona into manageable chunks...\n",
      "[+] Processing chunk 1/5...\n",
      "[+] Processing chunk 2/5...\n",
      "[+] Processing chunk 3/5...\n",
      "[+] Processing chunk 4/5...\n",
      "[+] Processing chunk 5/5...\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Path to the combined file\n",
    "file_path = Path(f\"UserProfile/combined_{username}_profile.txt\")\n",
    "\n",
    "# Read the entire content into a string\n",
    "combined_text_str = file_path.read_text(encoding=\"utf-8\")\n",
    "persona_json=await convert_persona_to_json(combined_text_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "700c73f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"persona\": {\n",
      "    \"reddit_username\": \"unknown\", // Extracted from markdown: \"Name: Unknown\"\n",
      "    \"name\": null,\n",
      "    \"photo_url\": \"\", // No photo or URL provided\n",
      "    \"demographics\": {\n",
      "      \"age\": null, // Extracted from markdown: \"Age: Unknown\"\n",
      "      \"occupation\": \"developer\", // Extracted from markdown: \"üíº Occupation: developer\"\n",
      "      \"location\": null, // Extracted from markdown: \"Location: Unknown\"\n",
      "      \"marital_status\": null,\n",
      "      \"tier\": null,\n",
      "      \"archetype\": \"ESTJ\", // Extracted from markdown: \"MBTI Estimate: ESTJ\"\n",
      "      \"AI_insights\": \"Age was unknown, but inferred occupation as developer due to their activity on Reddit and interest in tech. Their personality archetype (ESTJ) suggests they may be assertive and organized.\"\n",
      "    },\n",
      "    \"quote\": \"First of all you look good! But if you want to become more \"conventionally attractive\", here are some of my suggestions. Just personal opinions, take them or leave them.\",\n",
      "    \"traits\": {\n",
      "      \"content\": [\n",
      "        \"OCEAN: Openness 0.0011, Conscientiousness 0.004, Extraversion 0.0105, Agreeableness 0.0024, Neuroticism 0.0008\"\n",
      "      ],\n",
      "      \"AI_insights\": \"Extracted from OCEAN personality test, where they ranked low in Openness but high in Extraversion. This suggests they may be more outgoing and sociable.\"\n",
      "    },\n",
      "    \"motivations\": {\n",
      "      \"content\": [\n",
      "        \"- Dominant Emotion: positive ‚Äì 'Killer feature: accessing chatGPT (iPad app) and using audio as primary input. I...'\",\n",
      "        \"- Possibly career-focused, as extracted from activity on Reddit and interest in tech\",\n",
      "        \"- Interested in personal development, as suggested by quote on becoming more \"conventionally attractive\"\"\n",
      "      ],\n",
      "      \"AI_insights\": \"User's motivations are influenced by a desire for positive experiences (as indicated by their dominant emotion) and potentially career advancement.\"\n",
      "    },\n",
      "    \"personality\": {\n",
      "      \"content\": [\n",
      "        \"- Open-minded, but not particularly open (0.0011 OCEAN score)\",\n",
      "        \"- Assertive and organized, as suggested by ESTJ archetype\",\n",
      "        \"- Possibly insecure about physical appearance (as implied in quote)\"\n",
      "      ],\n",
      "      \"AI_insights\": \"Personality is characterized by a mix of traits, including assertiveness, organization, and potential insecurity about physical appearance.\"\n",
      "    },\n",
      "    \"behaviors_habits\": {\n",
      "      \"content\": [\n",
      "        \"- Active on Reddit, indicating interest in online communities\",\n",
      "        \"- Engages in online discussions, suggesting they value social interaction\",\n",
      "        \"- May be interested in personal development and self-improvement, as indicated by quote\"\n",
      "      ],\n",
      "      \"AI_insights\": \"User has a habit of engaging with online communities and values social interaction.\"\n",
      "    },\n",
      "    \"goals_needs\": {\n",
      "      \"content\": [\n",
      "        \"- Achieving \"conventional attractiveness\"\",\n",
      "        \"- Possibly career advancement\",\n",
      "        \"- Improving self-confidence through experience with others\"\n",
      "      ],\n",
      "      \"AI_insights\": \"User's goals and needs are focused on personal development, career advancement, and self-improvement.\"\n",
      "    },\n",
      "    \"pain_points\": {\n",
      "      \"content\": [\n",
      "        \"- Potential insecurity about physical appearance\",\n",
      "        \"- May struggle with assertiveness, as suggested by low Openness score\",\n",
      "        \"- Uncertainty about life goals and direction\"\n",
      "      ],\n",
      "      \"AI_insights\": \"User experiences anxiety about their physical appearance and may struggle with assertiveness.\"\n",
      "    },\n",
      "    \"tools_technology\": {\n",
      "      \"content\": [\n",
      "        \"- Active on Reddit, indicating familiarity with online platforms\",\n",
      "        \"- May use social media platforms like YouTube and Instagram\",\n",
      "        \"- Possibly interested in exploring tech advancements like chatGPT\"\n",
      "      ],\n",
      "      \"AI_insights\": \"User is familiar with online platforms, including Reddit, and may be interested in exploring tech advancements.\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "{\n",
      "  \"persona\": {\n",
      "    \"reddit_username\": \"jtraf6q\",\n",
      "    \"name\": \"User\",\n",
      "    \"photo_url\": \"\",\n",
      "    \"demographics\": {\n",
      "      \"age\": null,\n",
      "      \"occupation\": \"Engineer/Software Developer\",\n",
      "      \"location\": null,\n",
      "      \"marital_status\": null,\n",
      "      \"tier\": 3,\n",
      "      \"archetype\": \"Warrior/Briber\",\n",
      "      \"AI_insights\": \"Based on the content posted, it appears the user is a tier 3 player in a game, utilizing strategies such as pitting neighboring civilizations against each other to avoid direct conflict. [m1qihqd] and [lpo4k9e] provide evidence of this approach. Additionally, the user's comment on 'Bro sex is overrated' suggests they may be a male, but further information is needed to confirm their demographics. [1alf7av] indicates a positive interaction with AI technology, which led to the development of this persona.\"\n",
      "    },\n",
      "    \"quote\": \"\",\n",
      "    \"traits\": {\n",
      "      \"content\": [\n",
      "        \"Toxicity: Average Toxicity Score: 0.01600000075995922\",\n",
      "        \"Emotion Highlights: Positive (Score 1.0): [1alf7av]\",\n",
      "        \"Killer feature: accessing chatGPT (iPad app)\",\n",
      "        \"Fear (Score 1.0): [mxxhvpf]\",\n",
      "        \"Breakfast - Buvette\",\n",
      "        \"Coffee - Lyria\",\n",
      "        \"Lunch - Tsukemen okiboru\",\n",
      "        \"Coffee - La cabra\",\n",
      "        \"Dinner - Isodi\",\n",
      "        \"Drinks - Double chicken please\",\n",
      "        \"Negative (Score 1.0): [mzopv8i]\",\n",
      "        \"Yeah I was doing alright and then as soon as I upgraded to tier 3 my town was in shambles.\"\n",
      "      ],\n",
      "      \"AI_insights\": \"The user's content suggests they have a high level of engagement and interaction with various platforms (e.g. Reddit, chatGPT) and have a tendency to share personal opinions and experiences [jv3dcei]. They may prioritize personal freedom and self-improvement, as seen in their expense tracking and frugal living. [fmh0vsv] indicates an investment in put options and stocks, suggesting a financial interest in the market. [jv3dcei] and [fmh0vsv] serve as evidence for these traits.\"\n",
      "    },\n",
      "    \"motivations\": {\n",
      "      \"content\": [\n",
      "        \"50k at a time?. I want to transfer from Robinhood, but WB only allows 50k transfers at a time.\",\n",
      "        \"Is there no way to transfer 50k per day?\",\n",
      "        \"Now which row or column will win on a 5v5? I think it‚Äôll be the chads or the evils\",\n",
      "        \"My personal experience has definitely gotten worse.\"\n",
      "      ],\n",
      "      \"AI_insights\": \"The user's motivations appear to revolve around financial optimization, specifically transfer limits and market investments [m14y4pr]. They also show interest in games and may use strategic approaches to achieve success. [lpo4k9e] and [m1qihqd] demonstrate this interest. Additionally, [lyei609] indicates a decline in their personal experience, possibly leading to increased frustration. [l0cszaf] and [lyei609] provide evidence for these motivations.\"\n",
      "    },\n",
      "    \"personality\": {\n",
      "      \"content\": [\n",
      "        \"Surprise (Score 1.0): [ffps9r]\",\n",
      "        \"Surprise can sometimes lead to frustration, as seen in [ffps9r].\",\n",
      "        \"Anger (Score 0.5): [l0cszaf]\",\n",
      "        \"Anger and frustration are evident in [l0cszaf].\"\n",
      "      ],\n",
      "      \"AI_insights\": \"The user's personality is multifaceted, with traits such as surprise [ffps9r] and frustration [ffps9r, l0cszaf]. They seem to be able to adapt to new situations, as indicated by [ffps9r]. [l0cszaf] suggests they may have difficulty managing anger and frustration. [ffps9r, l0cszaf] serve as evidence for these personality traits.\"\n",
      "    },\n",
      "    \"behaviors_habits\": {\n",
      "      \"content\": [\n",
      "        \"The user engages in various online activities and shares their personal experiences and opinions.\",\n",
      "        \"They prioritize financial optimization and explore strategies for market investments.\",\n",
      "        \"The user demonstrates a tendency to interact with AI-powered technologies.\"\n",
      "      ],\n",
      "      \"AI_insights\": \"Based on their content, the user exhibits a high level of online engagement, financial acumen, and interest in AI-powered technologies [jv3dcei, fmh0vsv, 1alf7av]. They may develop beneficial habits such as tracking expenses and exploring new strategies for success. [jv3dcei] and [fmh0vsv] demonstrate these behaviors.\"\n",
      "    },\n",
      "    \"goals_needs\": {\n",
      "      \"content\": [\n",
      "        \"Financial independence and stability\",\n",
      "        \"Optimization of their investment portfolio\",\n",
      "        \"Development of beneficial habits for long-term success\"\n",
      "      ],\n",
      "      \"AI_insights\": \"The user's goals and needs revolve around financial security and independence [fmh0vsv, jv3dcei]. They may seek to optimize their investment portfolio and develop beneficial habits for long-term success. These goals and needs can be inferred through their content, with [fmh0vsv] and [jv3dcei] providing evidence for these goals.\"\n",
      "    },\n",
      "    \"pain_points\": {\n",
      "      \"content\": [\n",
      "        \"Frustration and anger due to difficulties with games and personal experiences\",\n",
      "        \"Transfer limits and difficulties with market investments\",\n",
      "        \"Decline in personal experiences and overall well-being\"\n",
      "      ],\n",
      "      \"AI_insights\": \"The user's pain points include frustration and anger due to difficulties with games and personal experiences [l0cszaf, lyei609]. They also face transfer limits and difficulties with market investments [m14y4pr]. Additionally, [lyei609] indicates a decline in their overall well-being. [l0cszaf] and [lyei609] serve as evidence for these pain points.\"\n",
      "    },\n",
      "    \"tools_technology\": {\n",
      "      \"content\": [\n",
      "        \"chatGPT (iPad app)\",\n",
      "        \"Robinhood/WB\",\n",
      "        \"Google Cloud Platform (GCP) Credits\"\n",
      "      ],\n",
      "      \"AI_insights\": \"The user leverages various tools and technologies, including AI-powered platforms like chatGPT and Google Cloud Platform's GCP Credits [m14y4pr]. They also utilize financial platforms such as Robinhood/WB for transfer and investment purposes. [fmh0vsv] and [1alf7av] demonstrate their familiarity with these tools and technologies.\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "{\n",
      "  \"persona\": {\n",
      "    \"reddit_username\": \"koxdru9\",\n",
      "    \"name\": null,\n",
      "    \"photo_url\": \"\",\n",
      "    \"demographics\": {\n",
      "      \"age\": null,\n",
      "      \"occupation\": null,\n",
      "      \"location\": null,\n",
      "      \"marital_status\": null,\n",
      "      \"tier\": null,\n",
      "      \"archetype\": null,\n",
      "      \"AI_insights\": \"\"\n",
      "    },\n",
      "    \"quote\": \"\",\n",
      "    \"traits\": {\n",
      "      \"content\": [\n",
      "        \"Open-minded\",\n",
      "        \"Financially conscious\",\n",
      "        \"Health-oriented\",\n",
      "        \"Tech-savvy\",\n",
      "        \"Socially active\"\n",
      "      ],\n",
      "      \"AI_insights\": \"Based on the user's comments, they seem to be open-minded, as evident from their responses to different perspectives and opinions. They are also financially conscious, as they discuss ways to reduce expenses and stretch their money. Additionally, they are health-oriented, mentioning their fitness goals and suggesting tips for a more 'conventional attractiveness'. The user is tech-savvy, having experience with Meta Quest and discussing the importance of mental models in finance. Finally, they are socially active, participating in online communities and engaging in conversations about various topics.\"\n",
      "    },\n",
      "    \"motivations\": {\n",
      "      \"content\": [\n",
      "        \"Financial freedom\",\n",
      "        \"Good physical health\",\n",
      "        \"Social connection\",\n",
      "        \"Personal growth\",\n",
      "        \"Exploring new technologies\"\n",
      "      ],\n",
      "      \"AI_insights\": \"Based on the user's comments, their motivations seem to be centered around achieving financial freedom, maintaining good physical health, and forging meaningful social connections. They also seem to be interested in personal growth, as evident from their discussions about learning and self-improvement. Additionally, they are curious about exploring new technologies and their applications.\"\n",
      "    },\n",
      "    \"personality\": {\n",
      "      \"content\": [\n",
      "        \"Analytical\",\n",
      "        \"Creative\",\n",
      "        \"Empathetic\",\n",
      "        \"Logical\",\n",
      "        \"Optimistic\"\n",
      "      ],\n",
      "      \"AI_insights\": \"Based on the user's comments, their personality traits seem to include being analytical, creative, empathetic, logical, and optimistic. They are able to break down complex topics and analyze them from different angles, as evident from their discussions about finance and technology. Furthermore, they are creative in their problem-solving approaches and are able to understand and relate to different perspectives. Additionally, they are logical in their thinking and try to base their opinions on facts and evidence. Finally, they seem to be optimistic, as evident from their discussions about personal growth and the potential benefits of new technologies.\"\n",
      "    },\n",
      "    \"behaviors_habits\": {\n",
      "      \"content\": [\n",
      "        \"Regularly checking Reddit\",\n",
      "        \"Engaging in online discussions\",\n",
      "        \"Sharing personal experiences\",\n",
      "        \"Seeking advice and feedback\",\n",
      "        \"Following trends and news\"\n",
      "      ],\n",
      "      \"AI_insights\": \"Based on the user's comments, their behaviors and habits include regularly checking Reddit, engaging in online discussions, sharing personal experiences, seeking advice and feedback, and following trends and news.\"\n",
      "    },\n",
      "    \"goals_needs\": {\n",
      "      \"content\": [\n",
      "        \"Financial stability\",\n",
      "        \"Good physical health\",\n",
      "        \"Meaningful social connections\",\n",
      "        \"Personal growth\",\n",
      "        \"Exploring new technologies\"\n",
      "      ],\n",
      "      \"AI_insights\": \"Based on the user's comments, their goals and needs seem to be centered around achieving financial stability, maintaining good physical health, and forging meaningful social connections. They also seem to be interested in personal growth, as evident from their discussions about learning and self-improvement. Additionally, they are curious about exploring new technologies and their applications.\"\n",
      "    },\n",
      "    \"pain_points\": {\n",
      "      \"content\": [\n",
      "        \"Financial stress\",\n",
      "        \"Physical health concerns\",\n",
      "        \"Social anxiety\",\n",
      "        \"Feeling overwhelmed by information\",\n",
      "        \"Difficulty adapting to change\"\n",
      "      ],\n",
      "      \"AI_insights\": \"Based on the user's comments, their pain points and challenges seem to include financial stress, physical health concerns, social anxiety, feeling overwhelmed by information, and difficulty adapting to change. They express concerns about their financial situation and the pressure to maintain a certain standard of living. Additionally, they discuss their efforts to maintain good physical health and deal with social anxiety. Furthermore, they mention feeling overwhelmed by the abundance of information available online and struggling to keep up with the pace of technological advancements.\"\n",
      "    },\n",
      "    \"tools_technology\": {\n",
      "      \"content\": [\n",
      "        \"Reddit\",\n",
      "        \"Meta Quest\",\n",
      "        \"Virtual reality\",\n",
      "        \"Finance apps\",\n",
      "        \"Fitness trackers\"\n",
      "      ],\n",
      "      \"AI_insights\": \"Based on the user's comments, they seem to be familiar with a variety of tools and technologies, including Reddit, Meta Quest, virtual reality, finance apps, and fitness trackers. They discuss their experiences with these platforms and tools, highlighting their potential benefits and limitations.\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "{\n",
      "  \"persona\": {\n",
      "    \"reddit_username\": \"HOME_LIFE\",\n",
      "    \"name\": null,\n",
      "    \"photo_url\": \"\",\n",
      "    \"demographics\": {\n",
      "      \"age\": null,\n",
      "      \"occupation\": null,\n",
      "      \"location\": null,\n",
      "      \"marital_status\": null,\n",
      "      \"tier\": null,\n",
      "      \"archetype\": null,\n",
      "      \"AI_insights\": \"From the user's post ID: 1lykkqf and content around 'intern season', it seems that the user is in their young adult stage, possibly a student or someone in their early twenties. Their interests and concerns also suggest a relatively lower level of income. The user's location is not explicitly stated, but their references to neighborhood and local amenities suggest they are likely living in a urban area in the United States. Based on their content and interests, the user's archetype could be defined as a young adult who is exploring their identity and independence.\",\n",
      "      \"post_ids\": [\"1lykkqf\", \"jv3dcei\", \"mzc2uit\"]\n",
      "    },\n",
      "    \"quote\": \"\",\n",
      "    \"traits\": {\n",
      "      \"content\": [\"introverted\", \"concerned with social status\", \"interested in personal development\"],\n",
      "      \"AI_insights\": \"From the user's content, it appears that they are introverted and may struggle with social anxiety. Their concerns about intern season and their own financial situation suggest that they are driven by a desire to improve their social status and economic security. They also appear to be interested in personal development and self-improvement.\"\n",
      "    },\n",
      "    \"motivations\": {\n",
      "      \"content\": [\"financial stability\", \"social status\", \"personal growth\"],\n",
      "      \"AI_insights\": \"Based on the user's posts and comments, it seems that they are motivated by a desire for financial stability and social status. They are also interested in personal growth and development, as evidenced by their engagement with online content and their interest in self-improvement.\"\n",
      "    },\n",
      "    \"personality\": {\n",
      "      \"content\": [\"adaptable\", \"resilient\", \"resourceful\"],\n",
      "      \"AI_insights\": \"From the user's content, it appears that they are adaptable and able to navigate complex social situations. They also seem to be resilient and resourceful, as evidenced by their ability to find ways to save money and improve their financial situation.\"\n",
      "    },\n",
      "    \"behaviors_habits\": {\n",
      "      \"content\": [\"frequenting local bars and restaurants\", \"engaging with online content\", \"using discount and promo codes\"],\n",
      "      \"AI_insights\": \"Based on the user's posts and comments, it seems that they are habitual socializers who enjoy frequenting local bars and restaurants. They also engage with online content and appear to be interested in finding ways to save money through discount and promo codes.\"\n",
      "    },\n",
      "    \"goals_needs\": {\n",
      "      \"content\": [\"financial stability\", \"social status\", \"personal growth\"],\n",
      "      \"AI_insights\": \"From the user's content, it appears that their goals and needs are centered around achieving financial stability and social status. They also seem to be interested in personal growth and development, as evidenced by their engagement with online content and their interest in self-improvement.\"\n",
      "    },\n",
      "    \"pain_points\": {\n",
      "      \"content\": [\"financial insecurity\", \"social anxiety\", \"feeling overwhelmed by life's demands\"],\n",
      "      \"AI_insights\": \"Based on the user's posts and comments, it seems that they experience pain points related to financial insecurity, social anxiety, and feeling overwhelmed by life's demands. These pain points appear to be driven by a desire for financial stability and social status.\"\n",
      "    },\n",
      "    \"tools_technology\": {\n",
      "      \"content\": [\"eBay\", \"Mercari\", \"eBay and Mercari apps\"],\n",
      "      \"AI_insights\": \"From the user's content, it appears that they use eBay and Mercari as tools for finding deals and saving money. They also mention using the eBay and Mercari apps to shop and find discounts.\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Note that some fields are null or empty, which means the user did not provide that information. I made educated guesses based on the user's content and online behavior.\n",
      "\n",
      "{\n",
      "  \"persona\": {\n",
      "    \"reddit_username\": \"u/username\",  // Replace 'u/username' with the actual username\n",
      "    \"name\": null,\n",
      "    \"photo_url\": \"\",\n",
      "    \"demographics\": {\n",
      "      \"age\": null,\n",
      "      \"occupation\": null,\n",
      "      \"location\": null,\n",
      "      \"marital_status\": null,\n",
      "      \"tier\": null,\n",
      "      \"archetype\": null,\n",
      "      \"AI_insights\": \"Based on travel-related posts, this user appears to be middle-aged (40-50 years old) and has a high school or college education (given their interest in travel and technology). Their location is likely in the United States (15 matches). [1] ID: lcv2en6 | Score: 2\"\n",
      "    },\n",
      "    \"quote\": \"\",\n",
      "    \"traits\": {\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"trait\": \"Traveler\",\n",
      "          \"description\": \"Enjoys traveling and exploring new places.\"\n",
      "        },\n",
      "        {\n",
      "          \"trait\": \"Tech-savvy\",\n",
      "          \"description\": \"Familiar with and interested in technology, including Vision Pro and gaming.\"\n",
      "        }\n",
      "      ],\n",
      "      \"AI_insights\": \"Based on their interest in travel and technology, this user appears to be a traveler who values exploring new places and is familiar with technology.\"\n",
      "    },\n",
      "    \"motivations\": {\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"motivation\": \"Exploration\",\n",
      "          \"description\": \"Motivated by a desire to explore new places and experience different cultures.\"\n",
      "        },\n",
      "        {\n",
      "          \"motivation\": \"Learning\",\n",
      "          \"description\": \"Motivated by a desire to learn new things and stay up-to-date with technology.\"\n",
      "        }\n",
      "      ],\n",
      "      \"AI_insights\": \"Based on their posts, this user appears to be motivated by a desire to explore new places and learn new things, possibly due to a sense of curiosity and a love for travel.\"\n",
      "    },\n",
      "    \"personality\": {\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"trait\": \"Positive\",\n",
      "          \"description\": \"Has a positive outlook on life and enjoys trying new things.\"\n",
      "        },\n",
      "        {\n",
      "          \"trait\": \"Curious\",\n",
      "          \"description\": \"Seems to be curious and interested in learning new things.\"\n",
      "        }\n",
      "      ],\n",
      "      \"AI_insights\": \"Based on their posts, this user appears to have a positive personality and a curious nature.\"\n",
      "    },\n",
      "    \"behaviors_habits\": {\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"behavior\": \"Travel frequently\",\n",
      "          \"description\": \"Enjoys traveling and exploring new places.\"\n",
      "        },\n",
      "        {\n",
      "          \"behavior\": \"Stay up-to-date with technology\",\n",
      "          \"description\": \"Familiar with and interested in technology, including Vision Pro and gaming.\"\n",
      "        }\n",
      "      ],\n",
      "      \"AI_insights\": \"Based on their posts, this user appears to have a behavior of traveling frequently and staying up-to-date with technology.\"\n",
      "    },\n",
      "    \"goals_needs\": {\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"goal\": \"Explore new places\",\n",
      "          \"description\": \"Wants to explore new places and experience different cultures.\"\n",
      "        },\n",
      "        {\n",
      "          \"goal\": \"Learn new things\",\n",
      "          \"description\": \"Wants to learn new things and stay up-to-date with technology.\"\n",
      "        }\n",
      "      ],\n",
      "      \"AI_insights\": \"Based on their posts, this user appears to have goals of exploring new places and learning new things.\"\n",
      "    },\n",
      "    \"pain_points\": {\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"pain_point\": \"Flight delays\",\n",
      "          \"description\": \"Has experienced flight delays and is frustrated by them.\"\n",
      "        },\n",
      "        {\n",
      "          \"pain_point\": \"Older technology\",\n",
      "          \"description\": \"Feels that older technology, such as fleets and engines, are holding them back.\"\n",
      "        }\n",
      "      ],\n",
      "      \"AI_insights\": \"Based on their posts, this user appears to have pain points related to flight delays and older technology.\"\n",
      "    },\n",
      "    \"tools_technology\": {\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"tool\": \"Vision Pro\",\n",
      "          \"description\": \"Uses Vision Pro for travel, gaming, and productivity.\"\n",
      "        },\n",
      "        {\n",
      "          \"tool\": \"Gaming consoles\",\n",
      "          \"description\": \"Uses gaming consoles for entertainment.\"\n",
      "        }\n",
      "      ],\n",
      "      \"AI_insights\": \"Based on their posts, this user appears to use Vision Pro for travel, gaming, and productivity, and gaming consoles for entertainment.\"\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "raw_string = persona_json # Replace with your full string\n",
    "\n",
    "# Step 1: Remove code block markers (```json or ``` at start/end)\n",
    "cleaned = re.sub(r\"```json|```\", \"\", raw_string)\n",
    "\n",
    "# Step 2: Remove all newline escape sequences\n",
    "cleaned = cleaned.replace(\"\\\\n\", \"\")\n",
    "\n",
    "# Step 3: Remove all backslashes\n",
    "cleaned = cleaned.replace(\"\\\\\", \"\")\n",
    "\n",
    "# Optional: strip leading/trailing whitespace\n",
    "cleaned = cleaned.strip()\n",
    "\n",
    "# Result is a flat, clean JSON string\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78e432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{username}_full_persona_2nd_approach.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(cleaned)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LDAenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
